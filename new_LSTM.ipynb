{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "new LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Phb6EtsJkikD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luBbG6HmnFs_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv(\"lstm400.8.csv\",engine ='python')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7MKJBCInP9S",
        "colab_type": "code",
        "outputId": "4e60b998-4a8c-4409-d8fb-419bb71dfbca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        }
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>...</th>\n",
              "      <th>362</th>\n",
              "      <th>363</th>\n",
              "      <th>364</th>\n",
              "      <th>365</th>\n",
              "      <th>366</th>\n",
              "      <th>367</th>\n",
              "      <th>368</th>\n",
              "      <th>369</th>\n",
              "      <th>370</th>\n",
              "      <th>371</th>\n",
              "      <th>372</th>\n",
              "      <th>373</th>\n",
              "      <th>374</th>\n",
              "      <th>375</th>\n",
              "      <th>376</th>\n",
              "      <th>377</th>\n",
              "      <th>378</th>\n",
              "      <th>379</th>\n",
              "      <th>380</th>\n",
              "      <th>381</th>\n",
              "      <th>382</th>\n",
              "      <th>383</th>\n",
              "      <th>384</th>\n",
              "      <th>385</th>\n",
              "      <th>386</th>\n",
              "      <th>387</th>\n",
              "      <th>388</th>\n",
              "      <th>389</th>\n",
              "      <th>390</th>\n",
              "      <th>391</th>\n",
              "      <th>392</th>\n",
              "      <th>393</th>\n",
              "      <th>394</th>\n",
              "      <th>395</th>\n",
              "      <th>396</th>\n",
              "      <th>397</th>\n",
              "      <th>398</th>\n",
              "      <th>399</th>\n",
              "      <th>400</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>508.3</td>\n",
              "      <td>556.3</td>\n",
              "      <td>559.4</td>\n",
              "      <td>584.0</td>\n",
              "      <td>632.4</td>\n",
              "      <td>656.0</td>\n",
              "      <td>612.0</td>\n",
              "      <td>562.4</td>\n",
              "      <td>537.7</td>\n",
              "      <td>526.2</td>\n",
              "      <td>525.5</td>\n",
              "      <td>534.0</td>\n",
              "      <td>532.5</td>\n",
              "      <td>537.9</td>\n",
              "      <td>543.1</td>\n",
              "      <td>544.2</td>\n",
              "      <td>568.2</td>\n",
              "      <td>601.7</td>\n",
              "      <td>625.7</td>\n",
              "      <td>631.9</td>\n",
              "      <td>594.8</td>\n",
              "      <td>554.2</td>\n",
              "      <td>533.5</td>\n",
              "      <td>532.4</td>\n",
              "      <td>539.3</td>\n",
              "      <td>541.1</td>\n",
              "      <td>532.9</td>\n",
              "      <td>541.5</td>\n",
              "      <td>559.1</td>\n",
              "      <td>593.4</td>\n",
              "      <td>612.1</td>\n",
              "      <td>627.4</td>\n",
              "      <td>599.7</td>\n",
              "      <td>557.6</td>\n",
              "      <td>541.9</td>\n",
              "      <td>543.8</td>\n",
              "      <td>525.0</td>\n",
              "      <td>524.4</td>\n",
              "      <td>523.1</td>\n",
              "      <td>532.8</td>\n",
              "      <td>...</td>\n",
              "      <td>580.0</td>\n",
              "      <td>590.0</td>\n",
              "      <td>574.7</td>\n",
              "      <td>560.5</td>\n",
              "      <td>556.8</td>\n",
              "      <td>553.1</td>\n",
              "      <td>553.4</td>\n",
              "      <td>559.4</td>\n",
              "      <td>549.6</td>\n",
              "      <td>550.9</td>\n",
              "      <td>553.2</td>\n",
              "      <td>547.6</td>\n",
              "      <td>554.4</td>\n",
              "      <td>569.1</td>\n",
              "      <td>575.3</td>\n",
              "      <td>572.9</td>\n",
              "      <td>562.4</td>\n",
              "      <td>556.2</td>\n",
              "      <td>552.3</td>\n",
              "      <td>550.1</td>\n",
              "      <td>549.6</td>\n",
              "      <td>554.8</td>\n",
              "      <td>555.8</td>\n",
              "      <td>552.8</td>\n",
              "      <td>550.2</td>\n",
              "      <td>549.4</td>\n",
              "      <td>549.8</td>\n",
              "      <td>541.2</td>\n",
              "      <td>546.6</td>\n",
              "      <td>550.8</td>\n",
              "      <td>550.9</td>\n",
              "      <td>550.5</td>\n",
              "      <td>553.3</td>\n",
              "      <td>555.2</td>\n",
              "      <td>568.6</td>\n",
              "      <td>582.7</td>\n",
              "      <td>578.9</td>\n",
              "      <td>565.4</td>\n",
              "      <td>569.5</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>561.4</td>\n",
              "      <td>553.1</td>\n",
              "      <td>560.2</td>\n",
              "      <td>553.5</td>\n",
              "      <td>545.9</td>\n",
              "      <td>552.5</td>\n",
              "      <td>549.8</td>\n",
              "      <td>566.1</td>\n",
              "      <td>586.5</td>\n",
              "      <td>574.5</td>\n",
              "      <td>561.8</td>\n",
              "      <td>559.5</td>\n",
              "      <td>551.2</td>\n",
              "      <td>556.4</td>\n",
              "      <td>557.3</td>\n",
              "      <td>543.1</td>\n",
              "      <td>542.7</td>\n",
              "      <td>557.6</td>\n",
              "      <td>549.4</td>\n",
              "      <td>552.3</td>\n",
              "      <td>552.9</td>\n",
              "      <td>549.3</td>\n",
              "      <td>553.1</td>\n",
              "      <td>554.0</td>\n",
              "      <td>547.1</td>\n",
              "      <td>552.1</td>\n",
              "      <td>551.5</td>\n",
              "      <td>553.7</td>\n",
              "      <td>557.6</td>\n",
              "      <td>557.1</td>\n",
              "      <td>569.2</td>\n",
              "      <td>582.7</td>\n",
              "      <td>581.4</td>\n",
              "      <td>564.9</td>\n",
              "      <td>561.3</td>\n",
              "      <td>571.1</td>\n",
              "      <td>566.7</td>\n",
              "      <td>549.8</td>\n",
              "      <td>552.7</td>\n",
              "      <td>553.2</td>\n",
              "      <td>...</td>\n",
              "      <td>575.1</td>\n",
              "      <td>562.8</td>\n",
              "      <td>562.7</td>\n",
              "      <td>555.3</td>\n",
              "      <td>553.1</td>\n",
              "      <td>550.1</td>\n",
              "      <td>548.8</td>\n",
              "      <td>558.6</td>\n",
              "      <td>556.2</td>\n",
              "      <td>553.5</td>\n",
              "      <td>558.2</td>\n",
              "      <td>552.4</td>\n",
              "      <td>538.0</td>\n",
              "      <td>556.6</td>\n",
              "      <td>553.1</td>\n",
              "      <td>553.3</td>\n",
              "      <td>574.9</td>\n",
              "      <td>584.6</td>\n",
              "      <td>593.9</td>\n",
              "      <td>585.9</td>\n",
              "      <td>562.0</td>\n",
              "      <td>555.8</td>\n",
              "      <td>562.3</td>\n",
              "      <td>557.1</td>\n",
              "      <td>555.7</td>\n",
              "      <td>559.3</td>\n",
              "      <td>560.4</td>\n",
              "      <td>558.7</td>\n",
              "      <td>552.6</td>\n",
              "      <td>547.0</td>\n",
              "      <td>547.8</td>\n",
              "      <td>554.2</td>\n",
              "      <td>562.5</td>\n",
              "      <td>592.2</td>\n",
              "      <td>594.4</td>\n",
              "      <td>580.7</td>\n",
              "      <td>566.3</td>\n",
              "      <td>561.4</td>\n",
              "      <td>554.1</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>553.9</td>\n",
              "      <td>550.5</td>\n",
              "      <td>557.8</td>\n",
              "      <td>558.6</td>\n",
              "      <td>547.6</td>\n",
              "      <td>550.0</td>\n",
              "      <td>556.7</td>\n",
              "      <td>565.5</td>\n",
              "      <td>573.6</td>\n",
              "      <td>597.4</td>\n",
              "      <td>586.6</td>\n",
              "      <td>569.1</td>\n",
              "      <td>571.3</td>\n",
              "      <td>559.9</td>\n",
              "      <td>554.5</td>\n",
              "      <td>554.2</td>\n",
              "      <td>548.8</td>\n",
              "      <td>551.7</td>\n",
              "      <td>554.9</td>\n",
              "      <td>552.3</td>\n",
              "      <td>559.8</td>\n",
              "      <td>574.1</td>\n",
              "      <td>582.8</td>\n",
              "      <td>589.0</td>\n",
              "      <td>582.2</td>\n",
              "      <td>579.7</td>\n",
              "      <td>557.7</td>\n",
              "      <td>546.7</td>\n",
              "      <td>550.8</td>\n",
              "      <td>551.4</td>\n",
              "      <td>565.2</td>\n",
              "      <td>558.5</td>\n",
              "      <td>548.1</td>\n",
              "      <td>559.5</td>\n",
              "      <td>552.2</td>\n",
              "      <td>543.2</td>\n",
              "      <td>546.7</td>\n",
              "      <td>554.6</td>\n",
              "      <td>567.8</td>\n",
              "      <td>581.8</td>\n",
              "      <td>...</td>\n",
              "      <td>549.8</td>\n",
              "      <td>562.8</td>\n",
              "      <td>562.2</td>\n",
              "      <td>571.5</td>\n",
              "      <td>572.9</td>\n",
              "      <td>583.5</td>\n",
              "      <td>586.8</td>\n",
              "      <td>571.7</td>\n",
              "      <td>570.6</td>\n",
              "      <td>554.0</td>\n",
              "      <td>549.3</td>\n",
              "      <td>562.0</td>\n",
              "      <td>554.9</td>\n",
              "      <td>546.1</td>\n",
              "      <td>545.6</td>\n",
              "      <td>549.4</td>\n",
              "      <td>553.8</td>\n",
              "      <td>571.0</td>\n",
              "      <td>585.3</td>\n",
              "      <td>587.4</td>\n",
              "      <td>601.5</td>\n",
              "      <td>575.3</td>\n",
              "      <td>541.4</td>\n",
              "      <td>532.6</td>\n",
              "      <td>555.9</td>\n",
              "      <td>567.4</td>\n",
              "      <td>543.7</td>\n",
              "      <td>501.0</td>\n",
              "      <td>500.1</td>\n",
              "      <td>547.1</td>\n",
              "      <td>588.1</td>\n",
              "      <td>592.8</td>\n",
              "      <td>555.4</td>\n",
              "      <td>548.3</td>\n",
              "      <td>322.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>479.0</td>\n",
              "      <td>530.6</td>\n",
              "      <td>538.2</td>\n",
              "      <td>597.9</td>\n",
              "      <td>663.7</td>\n",
              "      <td>655.9</td>\n",
              "      <td>598.4</td>\n",
              "      <td>534.1</td>\n",
              "      <td>504.6</td>\n",
              "      <td>513.2</td>\n",
              "      <td>521.7</td>\n",
              "      <td>512.8</td>\n",
              "      <td>510.6</td>\n",
              "      <td>522.1</td>\n",
              "      <td>531.7</td>\n",
              "      <td>556.9</td>\n",
              "      <td>618.8</td>\n",
              "      <td>663.7</td>\n",
              "      <td>664.5</td>\n",
              "      <td>611.5</td>\n",
              "      <td>538.4</td>\n",
              "      <td>509.5</td>\n",
              "      <td>510.7</td>\n",
              "      <td>517.2</td>\n",
              "      <td>528.2</td>\n",
              "      <td>523.9</td>\n",
              "      <td>527.7</td>\n",
              "      <td>533.8</td>\n",
              "      <td>568.3</td>\n",
              "      <td>631.9</td>\n",
              "      <td>664.0</td>\n",
              "      <td>647.9</td>\n",
              "      <td>591.8</td>\n",
              "      <td>529.7</td>\n",
              "      <td>516.5</td>\n",
              "      <td>515.2</td>\n",
              "      <td>516.8</td>\n",
              "      <td>519.9</td>\n",
              "      <td>529.3</td>\n",
              "      <td>527.0</td>\n",
              "      <td>...</td>\n",
              "      <td>624.5</td>\n",
              "      <td>569.5</td>\n",
              "      <td>524.8</td>\n",
              "      <td>522.4</td>\n",
              "      <td>524.9</td>\n",
              "      <td>526.2</td>\n",
              "      <td>525.5</td>\n",
              "      <td>528.8</td>\n",
              "      <td>536.3</td>\n",
              "      <td>540.0</td>\n",
              "      <td>562.3</td>\n",
              "      <td>629.3</td>\n",
              "      <td>662.6</td>\n",
              "      <td>641.9</td>\n",
              "      <td>596.4</td>\n",
              "      <td>541.0</td>\n",
              "      <td>519.5</td>\n",
              "      <td>519.4</td>\n",
              "      <td>531.9</td>\n",
              "      <td>524.0</td>\n",
              "      <td>526.9</td>\n",
              "      <td>534.4</td>\n",
              "      <td>539.3</td>\n",
              "      <td>546.9</td>\n",
              "      <td>595.2</td>\n",
              "      <td>650.0</td>\n",
              "      <td>659.8</td>\n",
              "      <td>629.9</td>\n",
              "      <td>559.1</td>\n",
              "      <td>517.8</td>\n",
              "      <td>518.8</td>\n",
              "      <td>526.2</td>\n",
              "      <td>526.1</td>\n",
              "      <td>527.9</td>\n",
              "      <td>530.9</td>\n",
              "      <td>534.6</td>\n",
              "      <td>536.6</td>\n",
              "      <td>573.9</td>\n",
              "      <td>638.8</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>522.9</td>\n",
              "      <td>528.8</td>\n",
              "      <td>525.3</td>\n",
              "      <td>535.2</td>\n",
              "      <td>539.4</td>\n",
              "      <td>532.6</td>\n",
              "      <td>540.7</td>\n",
              "      <td>594.9</td>\n",
              "      <td>652.9</td>\n",
              "      <td>661.0</td>\n",
              "      <td>628.4</td>\n",
              "      <td>566.7</td>\n",
              "      <td>528.8</td>\n",
              "      <td>513.2</td>\n",
              "      <td>520.8</td>\n",
              "      <td>526.1</td>\n",
              "      <td>533.6</td>\n",
              "      <td>539.1</td>\n",
              "      <td>531.2</td>\n",
              "      <td>530.3</td>\n",
              "      <td>551.8</td>\n",
              "      <td>615.8</td>\n",
              "      <td>661.9</td>\n",
              "      <td>653.1</td>\n",
              "      <td>603.9</td>\n",
              "      <td>549.4</td>\n",
              "      <td>518.6</td>\n",
              "      <td>518.1</td>\n",
              "      <td>524.9</td>\n",
              "      <td>525.9</td>\n",
              "      <td>529.0</td>\n",
              "      <td>532.7</td>\n",
              "      <td>538.8</td>\n",
              "      <td>546.1</td>\n",
              "      <td>572.3</td>\n",
              "      <td>634.4</td>\n",
              "      <td>657.6</td>\n",
              "      <td>636.1</td>\n",
              "      <td>582.4</td>\n",
              "      <td>533.3</td>\n",
              "      <td>...</td>\n",
              "      <td>525.0</td>\n",
              "      <td>527.5</td>\n",
              "      <td>528.3</td>\n",
              "      <td>537.6</td>\n",
              "      <td>535.7</td>\n",
              "      <td>538.6</td>\n",
              "      <td>588.7</td>\n",
              "      <td>648.0</td>\n",
              "      <td>658.9</td>\n",
              "      <td>629.0</td>\n",
              "      <td>578.3</td>\n",
              "      <td>528.2</td>\n",
              "      <td>520.2</td>\n",
              "      <td>524.5</td>\n",
              "      <td>528.9</td>\n",
              "      <td>531.8</td>\n",
              "      <td>535.3</td>\n",
              "      <td>538.9</td>\n",
              "      <td>538.1</td>\n",
              "      <td>557.7</td>\n",
              "      <td>620.5</td>\n",
              "      <td>656.0</td>\n",
              "      <td>647.0</td>\n",
              "      <td>603.5</td>\n",
              "      <td>555.3</td>\n",
              "      <td>522.2</td>\n",
              "      <td>519.1</td>\n",
              "      <td>522.4</td>\n",
              "      <td>526.2</td>\n",
              "      <td>529.3</td>\n",
              "      <td>534.3</td>\n",
              "      <td>537.9</td>\n",
              "      <td>540.0</td>\n",
              "      <td>584.2</td>\n",
              "      <td>642.1</td>\n",
              "      <td>659.9</td>\n",
              "      <td>630.9</td>\n",
              "      <td>583.7</td>\n",
              "      <td>533.8</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 401 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       1      2      3      4      5  ...    397    398    399    400  target\n",
              "0  508.3  556.3  559.4  584.0  632.4  ...  582.7  578.9  565.4  569.5     1.0\n",
              "1  561.4  553.1  560.2  553.5  545.9  ...  580.7  566.3  561.4  554.1     1.0\n",
              "2  553.9  550.5  557.8  558.6  547.6  ...    0.0    0.0    0.0    0.0     1.0\n",
              "3  479.0  530.6  538.2  597.9  663.7  ...  534.6  536.6  573.9  638.8     0.0\n",
              "4  522.9  528.8  525.3  535.2  539.4  ...  659.9  630.9  583.7  533.8     0.0\n",
              "\n",
              "[5 rows x 401 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yX1UNlpEQc-O",
        "colab_type": "text"
      },
      "source": [
        "資料切割"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ah7L5eGYQb2j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Dense, Dropout, LSTM, Embedding,MaxPooling1D,ConvLSTM2D,RNN\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "input_file = \"lstm400.10.csv\"\n",
        "\n",
        "def load_data(test_split = 0.2):\n",
        "    print ('Loading data...')\n",
        "    df = pd.read_csv(input_file)\n",
        "    X = df.iloc[:,:400].values\n",
        "    y = df.iloc[:,-1].values\n",
        "    min_max_scaler = preprocessing.MinMaxScaler()\n",
        "    X = min_max_scaler.fit_transform(X)\n",
        "    #X = preprocessing.scale(X)\n",
        "    print(X)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_split)\n",
        "    return pad_sequences(X_train), y_train, pad_sequences(X_test), y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYdSfk5yn3wM",
        "colab_type": "code",
        "outputId": "f440596f-dcc6-4565-aff5-b342ff9ada76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def create_model(input_length):\n",
        "    print ('Creating model...')\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim = 188, output_dim = 50, input_length = input_length))\n",
        "    model.add(LSTM(output_dim=256, activation='sigmoid', inner_activation='hard_sigmoid', return_sequences=True))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(MaxPooling1D(pool_size=2, strides=None, padding='valid', data_format='channels_last'))\n",
        "    model.add(LSTM(output_dim=256, activation='sigmoid', inner_activation='hard_sigmoid'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    print ('Compiling...')\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "\n",
        "X_train, y_train, X_test, y_test = load_data()\n",
        "\n",
        "model = create_model(len(X_train[0]))\n",
        "\n",
        "\n",
        "print ('Fitting model...')\n",
        "hist = model.fit(X_train, y_train, batch_size=1, nb_epoch=20, validation_split = 0.1, verbose = 1)\n",
        "\n",
        "score, acc = model.evaluate(X_test, y_test, batch_size=1)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "Creating model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(activation=\"sigmoid\", return_sequences=True, units=256, recurrent_activation=\"hard_sigmoid\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(activation=\"sigmoid\", units=256, recurrent_activation=\"hard_sigmoid\")`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Compiling...\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 200, 50)           9400      \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 200, 256)          314368    \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 200, 256)          0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1 (None, 50, 256)           0         \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, 256)               525312    \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 849,337\n",
            "Trainable params: 849,337\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Fitting model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:44: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 34 samples, validate on 4 samples\n",
            "Epoch 1/20\n",
            "34/34 [==============================] - 14s 404ms/step - loss: 1.0499 - acc: 0.4118 - val_loss: 0.7254 - val_acc: 0.5000\n",
            "Epoch 2/20\n",
            "34/34 [==============================] - 12s 359ms/step - loss: 0.7386 - acc: 0.5882 - val_loss: 0.6973 - val_acc: 0.5000\n",
            "Epoch 3/20\n",
            "34/34 [==============================] - 12s 360ms/step - loss: 0.7960 - acc: 0.4118 - val_loss: 0.6955 - val_acc: 0.5000\n",
            "Epoch 4/20\n",
            "34/34 [==============================] - 12s 361ms/step - loss: 0.8237 - acc: 0.4412 - val_loss: 0.6994 - val_acc: 0.5000\n",
            "Epoch 5/20\n",
            "34/34 [==============================] - 12s 360ms/step - loss: 0.8719 - acc: 0.4118 - val_loss: 0.7338 - val_acc: 0.5000\n",
            "Epoch 6/20\n",
            "34/34 [==============================] - 12s 360ms/step - loss: 0.6923 - acc: 0.5294 - val_loss: 0.7039 - val_acc: 0.5000\n",
            "Epoch 7/20\n",
            "34/34 [==============================] - 12s 357ms/step - loss: 0.8052 - acc: 0.4412 - val_loss: 0.6990 - val_acc: 0.5000\n",
            "Epoch 8/20\n",
            "34/34 [==============================] - 12s 363ms/step - loss: 0.8130 - acc: 0.2647 - val_loss: 0.6939 - val_acc: 0.5000\n",
            "Epoch 9/20\n",
            "34/34 [==============================] - 12s 366ms/step - loss: 0.7902 - acc: 0.4706 - val_loss: 0.6960 - val_acc: 0.5000\n",
            "Epoch 10/20\n",
            "34/34 [==============================] - 12s 362ms/step - loss: 0.7826 - acc: 0.5882 - val_loss: 0.7084 - val_acc: 0.5000\n",
            "Epoch 11/20\n",
            "34/34 [==============================] - 12s 362ms/step - loss: 0.7178 - acc: 0.5588 - val_loss: 0.6950 - val_acc: 0.5000\n",
            "Epoch 12/20\n",
            "34/34 [==============================] - 12s 367ms/step - loss: 0.7616 - acc: 0.4412 - val_loss: 0.6977 - val_acc: 0.5000\n",
            "Epoch 13/20\n",
            "34/34 [==============================] - 12s 360ms/step - loss: 0.7798 - acc: 0.4118 - val_loss: 0.7023 - val_acc: 0.5000\n",
            "Epoch 14/20\n",
            "34/34 [==============================] - 12s 362ms/step - loss: 0.7476 - acc: 0.5588 - val_loss: 0.6942 - val_acc: 0.5000\n",
            "Epoch 15/20\n",
            "34/34 [==============================] - 12s 363ms/step - loss: 0.6638 - acc: 0.6765 - val_loss: 0.6931 - val_acc: 0.5000\n",
            "Epoch 16/20\n",
            "34/34 [==============================] - 12s 364ms/step - loss: 0.6878 - acc: 0.5294 - val_loss: 0.6932 - val_acc: 0.5000\n",
            "Epoch 17/20\n",
            "34/34 [==============================] - 12s 366ms/step - loss: 0.7578 - acc: 0.3824 - val_loss: 0.6982 - val_acc: 0.5000\n",
            "Epoch 18/20\n",
            "34/34 [==============================] - 13s 368ms/step - loss: 0.7348 - acc: 0.5000 - val_loss: 0.7046 - val_acc: 0.5000\n",
            "Epoch 19/20\n",
            "34/34 [==============================] - 13s 368ms/step - loss: 0.7054 - acc: 0.5000 - val_loss: 0.6932 - val_acc: 0.5000\n",
            "Epoch 20/20\n",
            "34/34 [==============================] - 13s 376ms/step - loss: 0.7878 - acc: 0.4118 - val_loss: 0.6936 - val_acc: 0.5000\n",
            "10/10 [==============================] - 1s 135ms/step\n",
            "Test score: 0.6997584104537964\n",
            "Test accuracy: 0.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SecFMBWSsqJQ",
        "colab_type": "code",
        "outputId": "1a34d820-3b09-4a7d-89d3-5762c655bde0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "X_train, y_train, X_test, y_test = load_data()\n",
        "import numpy as np\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
        "y_train = np.reshape(y_train, (y_train.shape[0], 1))\n",
        "y_test = np.reshape(y_test, (y_test.shape[0], 1))\n",
        "print(X_train.shape,X_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "[[0.73893199 0.7406532  0.55372204 ... 0.53861183 0.5264432  0.53031008]\n",
            " [0.86010954 0.7337774  0.55518885 ... 0.52688872 0.52271881 0.51596983]\n",
            " [0.84299407 0.7281908  0.55078841 ... 0.         0.         0.        ]\n",
            " ...\n",
            " [0.79735281 0.68650623 0.50220022 ... 0.58289914 0.59050279 0.57016482]\n",
            " [0.77247832 0.68908466 0.50770077 ... 0.52446967 0.5094041  0.50041903]\n",
            " [0.80876312 0.70176193 0.56710671 ... 0.49916263 0.4981378  0.49939473]]\n",
            "(92, 400, 1) (23, 400, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3X3sLt0_uYK",
        "colab_type": "code",
        "outputId": "392cc774-754a-4def-b693-3b8749a18935",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "from keras.callbacks import Callback\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.models import Sequential\n",
        "from numpy.random import choice\n",
        "\n",
        "\n",
        "USE_SEQUENCES = False\n",
        "USE_STATELESS_MODEL = False\n",
        "\n",
        "# you can all the four possible combinations\n",
        "# USE_SEQUENCES and USE_STATELESS_MODEL\n",
        "\n",
        "max_len = 400\n",
        "batch_size = 1\n",
        "\n",
        "class ResetStatesCallback(Callback):\n",
        "    def __init__(self):\n",
        "        self.counter = 0\n",
        "\n",
        "    def on_batch_begin(self, batch, logs={}):\n",
        "        if self.counter % max_len == 0:\n",
        "            self.model.reset_states()\n",
        "        self.counter += 1\n",
        "\n",
        "print('sequences_x_train shape:', X_train.shape)\n",
        "print('sequences_y_train shape:', y_train.shape)\n",
        "\n",
        "print('sequences_x_test shape:', X_test.shape)\n",
        "print('sequences_y_test shape:', y_test.shape)\n",
        "\n",
        "if USE_STATELESS_MODEL:\n",
        "    print('Build STATELESS model...')\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(10, input_shape=(max_len, 1), return_sequences=True))\n",
        "    model.add(LSTM(10, input_shape=(max_len, 1), return_sequences=False))\n",
        "    model.add(Dense(1, optimizer='rmsprop'))\n",
        "\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    print('Train...')\n",
        "    model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=15,\n",
        "              validation_data=(X_test, y_test), shuffle=False, callbacks=[ResetStatesCallback()])\n",
        "\n",
        "    score, acc = model.evaluate(X_test, y_test, batch_size=batch_size, verbose=0)\n",
        "    print('___________________________________')\n",
        "    print('Test score:', score)\n",
        "    print('Test accuracy:', acc)\n",
        "else:\n",
        "    # STATEFUL MODEL\n",
        "    print('Build STATEFUL model...')\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(10,batch_input_shape=(1, 1, 1), return_sequences=False,stateful=True))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    x = np.expand_dims(np.expand_dims(X_train.flatten(), axis=1), axis=1)\n",
        "    y = np.expand_dims(np.array([[v] * max_len for v in y_train.flatten()]).flatten(), axis=1)\n",
        "    model.fit(x,\n",
        "              y,\n",
        "              callbacks=[ResetStatesCallback()],\n",
        "              batch_size=1,\n",
        "              shuffle=False)\n",
        "\n",
        "    print('Train...')\n",
        "    for epoch in range(15):\n",
        "        mean_tr_acc = []\n",
        "        mean_tr_loss = []\n",
        "        for i in range(len(X_train)):\n",
        "            y_true = y_train[i]\n",
        "            for j in range(max_len):\n",
        "                tr_loss, tr_acc = model.train_on_batch(np.expand_dims(np.expand_dims(X_train[i][j], axis=1), axis=1),\n",
        "                                                       np.array([y_true]))\n",
        "                mean_tr_acc.append(tr_acc)\n",
        "                mean_tr_loss.append(tr_loss)\n",
        "            model.reset_states()\n",
        "\n",
        "        print('accuracy training = {}'.format(np.mean(mean_tr_acc)))\n",
        "        print('loss training = {}'.format(np.mean(mean_tr_loss)))\n",
        "        print('___________________________________')\n",
        "\n",
        "        mean_te_acc = []\n",
        "        mean_te_loss = []\n",
        "        for i in range(len(X_test)):\n",
        "            for j in range(max_len):\n",
        "                te_loss, te_acc = model.test_on_batch(np.expand_dims(np.expand_dims(X_test[i][j], axis=1), axis=1),\n",
        "                                                      y_test[i])\n",
        "                mean_te_acc.append(te_acc)\n",
        "                mean_te_loss.append(te_loss)\n",
        "            model.reset_states()\n",
        "\n",
        "            for j in range(max_len):\n",
        "                y_pred = model.predict_on_batch(np.expand_dims(np.expand_dims(X_test[i][j], axis=1), axis=1))\n",
        "            model.reset_states()\n",
        "\n",
        "        print('accuracy testing = {}'.format(np.mean(mean_te_acc)))\n",
        "        print('loss testing = {}'.format(np.mean(mean_te_loss)))\n",
        "        print('___________________________________')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sequences_x_train shape: (92, 400, 1)\n",
            "sequences_y_train shape: (92, 1)\n",
            "sequences_x_test shape: (23, 400, 1)\n",
            "sequences_y_test shape: (23, 1)\n",
            "Build STATEFUL model...\n",
            "Epoch 1/1\n",
            "   30/36800 [..............................] - ETA: 51:48 - loss: 0.6608 - acc: 0.9667  "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:95: RuntimeWarning: Method (on_train_batch_begin) is slow compared to the batch update (0.929320). Check your callbacks.\n",
            "  % (hook_name, delta_t_median), RuntimeWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "36800/36800 [==============================] - 207s 6ms/step - loss: 0.1956 - acc: 0.9111\n",
            "Train...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x71zbJbuAMu8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}